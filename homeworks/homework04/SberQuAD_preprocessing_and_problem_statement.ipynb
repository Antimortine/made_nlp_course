{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: the provided initial code is an adaptation of the [Starter code for Stanford CS224n default final project on SQuAD 2.0](https://github.com/chrischute/squad) which is shared under MIT License. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does initial preprocessing for the SberQuAD dataset and will give you the starting point in this assignment. If it looks too complex and/or time/resourse-expensive, you may stick to homework05 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "This code is a bit changed version of the code from `setup.py`. If you want to work with the SQuAD dataset, stick to the original instructions from the https://github.com/chrischute/squad repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train a model on SQuAD.\n",
    "\n",
    "Author:\n",
    "    Chris Chute (chute@stanford.edu)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import util\n",
    "\n",
    "from args import get_train_args\n",
    "from collections import OrderedDict\n",
    "from json import dumps\n",
    "from models import BiDAF\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from util import collate_fn, SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./save\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the SberQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-16 12:58:06--  http://files.deeppavlov.ai/datasets/sber_squad_clean-v1.1.tar.gz\n",
      "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 93.175.29.74\n",
      "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|93.175.29.74|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22766184 (22M) [application/octet-stream]\n",
      "Saving to: ‘./data/sber_squad_clean-v1.1.tar.gz’\n",
      "\n",
      "./data/sber_squad_c 100%[===================>]  21.71M  5.39MB/s    in 3.7s    \n",
      "\n",
      "2020-06-16 12:58:09 (5.84 MB/s) - ‘./data/sber_squad_clean-v1.1.tar.gz’ saved [22766184/22766184]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://files.deeppavlov.ai/datasets/sber_squad_clean-v1.1.tar.gz -nc -O ./data/sber_squad_clean-v1.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-v1.1.json\n",
      "dev-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "! tar -xzvf ./data/sber_squad_clean-v1.1.tar.gz\n",
    "! mv train-v1.1.json data\n",
    "! mv dev-v1.1.json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-16 13:00:35--  http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\n",
      "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 93.175.29.74\n",
      "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|93.175.29.74|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4110986108 (3.8G) [application/octet-stream]\n",
      "Saving to: ‘./data/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec’\n",
      "\n",
      "ft_native_300_ru_wi   0%[                    ]  13.53M  4.11MB/s    eta 16m 56s^C\n"
     ]
    }
   ],
   "source": [
    "! wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec -nc -O ./data/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the preprocessing for the SberQuAD dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train-v1.1.json'\n",
    "dev_file = './data/dev-v1.1.json'\n",
    "glove_file = './data/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if needed\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell may take a while (usually 10 minutes or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:48<00:00, 108.28s/it]\n",
      "  0%|          | 903/1560132 [00:00<02:52, 9023.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45328 questions in total\n",
      "Pre-processing word vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560132/1560132 [02:40<00:00, 9696.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135451 / 156143 tokens have corresponding word embedding vector\n",
      "Pre-processing char vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 tokens have corresponding char embedding vector\n",
      "Pre-processing dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5036 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process training set and use it to decide on the word/character vocabularies\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "train_examples, train_eval = process_file(train_file, \"train\", word_counter, char_counter, nlp)\n",
    "word_emb_mat, word2idx_dict = get_embedding(\n",
    "    word_counter, 'word', emb_file=glove_file, vec_size=300, num_vectors=1560132)\n",
    "char_emb_mat, char2idx_dict = get_embedding(\n",
    "    char_counter, 'char', emb_file=None, vec_size=64)\n",
    "\n",
    "\n",
    "dev_examples, dev_eval = process_file(dev_file, \"dev\", word_counter, char_counter, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_file = './data/train.npz'\n",
    "dev_record_file = './data/dev.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import add_common_args, get_setup_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving the default arguments for the preprocessing script\n",
    "_args = get_setup_args(bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(ans_limit=30, answer_file='./data/answer.json', char2idx_file='./data/char2idx.json', char_dim=64, char_emb_file='./data/char_emb.json', char_limit=16, dev_eval_file='./data/dev_eval.json', dev_meta_file='./data/dev_meta.json', dev_record_file='./data/dev.npz', dev_url='https://github.com/chrischute/squad/data/dev-v2.0.json', glove_dim=300, glove_num_vecs=2196017, glove_url='http://nlp.stanford.edu/data/glove.840B.300d.zip', include_test_examples=True, para_limit=400, ques_limit=50, test_eval_file='./data/test_eval.json', test_meta_file='./data/test_meta.json', test_para_limit=1000, test_ques_limit=100, test_record_file='./data/test.npz', test_url='https://github.com/chrischute/squad/data/test-v2.0.json', train_eval_file='./data/train_eval.json', train_record_file='./data/train.npz', train_url='https://github.com/chrischute/squad/data/train-v2.0.json', word2idx_file='./data/word2idx.json', word_emb_file='./data/word_emb.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [00:00, 2917.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train examples to indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45328it [00:14, 3226.18it/s]\n",
      "312it [00:00, 3115.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 45213 / 45328 instances of features in total\n",
      "Converting dev examples to indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5036it [00:01, 3237.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 5022 / 5036 instances of features in total\n"
     ]
    }
   ],
   "source": [
    "build_features(_args, train_examples, \"train\", train_record_file, word2idx_dict, char2idx_dict)\n",
    "dev_meta = build_features(_args, dev_examples, \"dev\", dev_record_file, word2idx_dict, char2idx_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving word embedding...\n",
      "Saving char embedding...\n",
      "Saving train eval...\n",
      "Saving dev eval...\n",
      "Saving word dictionary...\n",
      "Saving char dictionary...\n",
      "Saving dev meta...\n"
     ]
    }
   ],
   "source": [
    "save(_args.word_emb_file, word_emb_mat, message=\"word embedding\")\n",
    "save(_args.char_emb_file, char_emb_mat, message=\"char embedding\")\n",
    "save(_args.train_eval_file, train_eval, message=\"train eval\")\n",
    "save(_args.dev_eval_file, dev_eval, message=\"dev eval\")\n",
    "save(_args.word2idx_file, word2idx_dict, message=\"word dictionary\")\n",
    "save(_args.char2idx_file, char2idx_dict, message=\"char dictionary\")\n",
    "save(_args.dev_meta_file, dev_meta, message=\"dev meta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The experiment\n",
    "\n",
    "Now you are almost ready to go. You may follow these steps to begin (or just start your experiments here).\n",
    "\n",
    "1. Try running the `train.py` script from the console (or via `!`) (default command-line arguments are ok for the start). If will run the BiDAF model on the preprocessed data. Set `--use_squad_v2` flag to False (SberQuAD is similar to SQuAD v1.1).\n",
    "\n",
    "Example code (be careful with the path and the names of the variables):\n",
    "```\n",
    "python train.py --name first_run_on_sberquad --use_squad_v2 False\n",
    "```\n",
    "\n",
    "2. After if finishes (might take an 1-2-3 hours depending on the hardware), evaluate your model on the `dev` set and measure the quality.\n",
    "Example code (be careful with the path and the names of the variables):\n",
    "```\n",
    " python test.py --split dev --load_path ./save/train/first_run_on_sberquad-02/best.pth.tar --name best_evaluation_experiment\n",
    "```\n",
    "The result should be similar to the following:\n",
    "```\n",
    ">>> Dev NLL: 04.94, F1: 49.22, EM: 30.53, AvNA: 97.68\n",
    "```\n",
    "\n",
    "The [DeepPavlov's RuBERT](http://docs.deeppavlov.ai/en/master/features/models/squad.html) achieves $EM = 66.30\\pm0.24$ and $F1 = 84.60\\pm0.11$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here comes your quest: try to improve the quality of this QA system. \n",
    "\n",
    "This is a very creative assignment. It is all about experimenting, trying different approaches (and a lot of computations). But if you wish to stick to some numbers, try to increase F1 at least by $5$ points.\n",
    "\n",
    "Here are some ideas that might help you on your way:\n",
    "* Try adapting the optimization hyperparameters/network structure to Russian language (the baseline is designed for English SQuAD dataset).\n",
    "* Incorporating the additional information about the data (like PoS tags) might be a good idea.\n",
    "* __Distilling the knowledge from a pre-trained RuBERT__ (e.g. try to use the predictions of the model we've discussed on `week10` as soft targets).\n",
    "* Or anything else.\n",
    "\n",
    "\n",
    "And, first of all, read the initial code carefully.\n",
    "\n",
    "\n",
    "Good luck! Feel free to share your results :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
